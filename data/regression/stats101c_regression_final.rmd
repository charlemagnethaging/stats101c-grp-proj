---
title: "Homework 7"
author: "Angela Luo"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
header-includes:
  - \usepackage{xcolor}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width=7, fig.height=5)
library(palmerpenguins)
data(penguins)
library(rpart)
library(mlr3verse)
library(ggplot2)
library(modeldata)
data(mlc_churn)
library(xgboost)
library(iml)
```

## Question 1a
```{r, fig.width=10, fig.height=6}
## removing missing values
penguins <- penguins[!is.na(penguins$body_mass_g), ]
df1 <- penguins
tr <- rpart(body_mass_g ~ ., data = penguins)
print(tr)
tr |> rpart.plot::rpart.plot()
```
Starting at the top, a penguin's weight of 4204g is predicted on the entire data set depending on whether the penguin's species is Adelie or Chinstrap. 

If a penguin is Adelie or Chinstrap, we go to an internal node representing 64% of observations that land in this internal node. Penguins that are Adelie or Chinstrap are predicted to weight 3711g. If a penguin is Adelie or Chinstrap and a female, then we go to a terminal node representing 32% of observations of penguins that are Adelie or Chinstrap. Within this terminal node, female penguins that are Adelie or Chinstrap will weight 3421g. If a penguin is Adelie or Chinstrap and a male, then we go to a terminal node representing 32% of observations of penguins that are Adelie or Chinstrap. Within this terminal node, male penguins that are Adelie or Chinstrap will weight 3998g. 


If a penguin is not Adelie or Chinstrap, we go to an internal node representing 36% of observations that land in this internal node. Penguins that are not Adelie or Chinstrap are predicted to weight 5076g. If a penguin is not Adelie or Chinstrap and a female, then we go to a terminal node representing 18% of observations of penguins that are not Adelie or Chinstrap. Within this terminal node, female penguins that are not Adelie or Chinstrap will weight 4670g. If a penguin is not Adelie or Chinstrap and a male, then we go to a terminal node representing 18% of observations of penguins that are not Adelie or Chinstrap. Within this terminal node, male penguins that are not Adelie or Chinstrap will weight 5475g. 

## Question 1b
```{r}
#rss
residuals <- residuals(tr)
rss <- sum(residuals^2)

# mse
mse <- rss / nrow(penguins)

#rmse
rmse <- sqrt(mse)

#var 
var_w <- var(penguins$body_mass_g)

1 - (mse/var_w)
```
The variance explained value is pretty close to 1 with a value of 0.84 so the model is a fairly strong predictive model. 

## Question 2a
```{r, fig.width=10, fig.height=6}
tsk_species <- as_task_classif(penguins, target = "species") # define task
class_tree <- lrn("classif.rpart", predict_type = "prob") # don't predict labels
class_tree$train(tsk_species) # train on actual data
class_tree$model # the internal rpart model, some nodes were pruned

class_tree$model |> rpart.plot::rpart.plot()
```
Starting at the top, a penguin's species being Adelie is predicted on the entire data set depending on whether the penguin's flipper length is greater or less than 207mm. 44% of the data would be Adelie, 20% would be Chinstrap, and 36% would be Gentoo. 

If a penguin's flipper length is less than 207mm, we go to an internal node representing 62% of observations that land in this internal node. This internal node decides upon the species being Adelie based on the bill length with 70% of the data being Adelie and 30% being Chinstrap. If the bill length is less than 43mm, then we go to a terminal node representing 44% of observations of penguins that have a flipper length is less than 207mm. This terminal node decides that the species is Adelie for 97% of Adelie and 3% of Chinstrap penguins that have a flipper length less than 207mm and bill length less than 43mm. If the bill length is greater than 43mm, then we go to a terminal node representing 18% of observations of penguins that have a flipper length less than 207mm. This terminal node decides that the species is Chinstrap for 6% of Adelie, 92% of Chinstrap, and 2% of Gentoo penguins that have a flipper length less than 207mm and bill length greater than 43mm. 

If a penguin's flipper length is greater than 207mm, we go to an internal node representing 38% of observations that land in this internal node. This internal node decides upon the species being Gentoo based on the bill length with 2% of the data being Adelie, 4% being Chinstrap, and 95% being Gentoo. If the bill depth is greater than 18mm, then we go to a terminal node representing 2% of observations of penguins that have a flipper length is greater than 207mm. This terminal node decides that the species is Chinstrap for 29% of Adelie and 71% of Chinstrap penguins that have a flipper length greater than 207mm and bill depth greater than 18mm. If the bill depth is less than 18mm, then we go to a terminal node representing 36% of observations of penguins that have a flipper length greater than 207mm. This terminal node decides that the species is Gentoo for 100% of Gentoo penguins that have a flipper length greater than 207mm and bill length less than 18mm. 

## Question 2b
```{r}
pred <- class_tree$predict(tsk_species)

# brier score
brier <- pred$score(msr("classif.mbrier"))
# accuracy
accuracy <- pred$score(msr("classif.acc"))

cat("The brier score is:", brier, "\n")
cat("The accuracy score is:", accuracy, "\n")
```
Our brier score is very small and close to zero with a value of 0.06, meaning that our model is a strong predictive model. Additionally, our accuracy score is very close to 1 with a value of 0.96, meaning that our model had a high proportion of correct predictions which supports our claim of our model being a strong predictive model. 

## Question 3a
```{r, fig.width=10, fig.height=6}
tsk_weight <- as_task_regr(penguins, target = "body_mass_g") # define task
class_rf <- lrn("regr.ranger", importance = "impurity") # compute variable importance
class_rf$train(task = tsk_weight) # train on penguin data
class_rf$model # the internal ranger model

## variable importance plot
# extract variable importance
importance_scores <- class_rf$model$model$variable.importance
ggplot(data = data.frame(var = names(importance_scores), value = importance_scores)) +
  geom_point(aes(x = value, y = reorder(var, value))) + ylab("variable") + 
  xlab("importance") 
```

## Question 3b
```{r}
pred <- class_rf$predict(tsk_weight)

# mse
mse <- as.numeric(pred$score(msr("regr.mse")))

#rmse
cat("Checking the RMSE is exactly equal to the square-root of the MSE:", "\n")
cat("Square-root of the MSE:", sqrt(mse), "\n")
cat("regr.mse:", as.numeric(pred$score(msr("regr.rmse"))), "\n")

#var 
var_w <- var(penguins$body_mass_g)

1 - (mse/var_w)
```
The variance explained value is very close to 1 with a value of 0.95 so the model is a very strong predictive model. 

## Question 4a
```{r}
levels(mlc_churn$churn)

tsk_churn <- as_task_classif(mlc_churn, target = "churn") # define task
```

## Question 4b
```{r}
lrn_xgb <- as_learner(po("encode", method = "one-hot") %>>% 
                        lrn("classif.xgboost", predict_type = "prob"))
lrn_xgb$train(task = tsk_churn)

lrn_xgb$model$classif.xgboost$model
```
## Question 4c
```{r}
rsmp_cv5 <- rsmp("cv", folds = 5) 
msrs = c(msr("classif.mbrier"), msr("classif.acc"))

set.seed(123) # we randomly assign to folds

res_xgb <- resample(
  task = tsk_churn,
  learner = lrn_xgb,
  resampling = rsmp_cv5
)

cv_results <- res_xgb$aggregate(measures = c(msr("classif.mbrier"), msr("classif.acc")))
print(cv_results)
```
Our brier score is very small and close to zero with a value of 0.07, meaning that our model is a strong predictive model. Additionally, our accuracy score is very close to 1 with a value of 0.96, meaning that our model had a high proportion of correct predictions which supports our claim of our model being a strong predictive model.

## Question 4d
```{r, fig.width=10, fig.height=6}
learners = lrns(c("classif.rpart", "classif.ranger", "classif.featureless"), predict_type = "prob")
learners$encode.classif.xgboost = as_learner(
  po("encode", method = "one-hot") %>>% lrn("classif.xgboost"))
set.seed(123) # we randomly assign to folds
bmr_design <- benchmark_grid(tasks = tsk_churn,
learners = learners, resamplings = rsmp_cv5)

bmr <- benchmark(design = bmr_design)
bmr$aggregate(measures = c(msr("classif.mbrier"), msr("classif.acc")))[,c("learner_id", "classif.mbrier", "classif.acc")]

autoplot(bmr) + scale_y_log10()
```
We see that the random forest model and xgboost model produce the smallest classification error, which makes sense since these are the model expected to perform the best. I'm not sure why is doesnt show in the table above, but from our answer in 4c, the xgboost model produces the smallest brier score and is tied with the random forest model for the accuracy score. This makes sense with what we see in our visual of the model performances. The featureless performed the worse and we can see that from the numeric values being the worst and in the visual. The rpart model is then a bit better with a decently low brier score and fairly high accuracy score, but we still can see the random forest model and xgboost models are better. 

