---
title: "Stats 101C Regression Final"
author: "Group 13"
date: "`r Sys.Date()`"
output:
  pdf:
    toc: true
    toc_depth: 3
  html:
    toc: true
    toc_depth: 3
embed-resources: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width=7, fig.height=5)
library(tidyverse)
library(rpart)
library(mlr3verse)
library(xgboost)
library(broom)
library(glmnet)
```

```{r}
# Load data
load("processed_data_train.RData")
```

# Introduction 

Research shows that factors such as income, age, gender, and household composition often influence spending behavior and product choice (U.S. Bureau of Labor Statistics). This background makes us interested to see if online spending is associated with variables such as income, education, age, and household sizes. The household size could be associated with how often and how many products are typically purchased but the different demographics of the customer or household could be associated with the purchase category. Personal habits and factors related to personal health could also place an influential role in purchasing behavior. These variables are all plausible predictors of online spending behavior.


# Preprocessing (NEED TO UPDATE)

```{r}

```

## Exploratory Data Analysis
As we explored potential relationships between the variables in the training dataset, notable patterns were discovered and some failed to show. Our findings are presented below: 

### Response (HH Size) Distribution

```{r, fig.width=10, fig.height=6}
set_theme(theme_minimal())

ggplot(data, aes(x = q.amazon.use.hh.size.num, fill=q.amazon.use.hh.size.num, group=as.factor(q.amazon.use.hh.size.num))) +
  geom_bar() +
  scale_fill_gradient(low="#ccebc5", high="#08589e") +
  labs(title = "Distribution of Housing Size",
       x = "Housing Size",
       y = "Count"
       ) +
  theme(legend.position="none")
```

Just looking at the distribution of housing sizes, we see that it's most common for households to have a size of 2, with size 4+ or 1 the second most frequent, and a size of 3 having the lowest frequency. Since household sizes of 4 and more are represented by only one variable, it's difficult to tell what the breakdown of household sizes are within that variable. 

### Numeric Variable Correlation

```{r, fig.width=10, fig.height=6}
numeric_data <- data |> 
  select(where(is.numeric))

# Compute correlation matrix
cor_matrix <- cor(numeric_data, use = "pairwise.complete.obs")

# Convert to long format for ggplot
cor_long <- as.data.frame(as.table(cor_matrix))

ggplot(cor_long, aes(Var1, Var2, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "darkgreen", high = "pink", mid = "white",
                       midpoint = 0, limit = c(-1,1), space = "Lab",
                       name = "Correlation") +
  geom_text(aes(label = round(Freq, 2)), color = "black", size = 4) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(title = "Correlation Matrix Heatmap (Numeric Variables)", x = "", y = "")
```

From our correlation heat map, we notice that all the numerical variables do not show any kind of correlation among each other. Surprisingly the amount of products purchased and the number of users per amazon account doesn't have any correlation to the household size. The variable interactions seem to be statistically negligible in terms of a linear relationship. 


### Location

```{r, fig.width=10, fig.height=6}
ggplot(data, aes(y = fct_rev(fct_infreq(shipping.address.state)))) +
	geom_bar(fill = "#7ec17bff") +
	labs(
		title = "Distribution of Shipping Address",
		y = "Shipping Address State"
	)

# top_25_states <- data |>
# 	count(shipping.address.state) |>
# 	slice_max(n, n = 25) |>
# 	pull(shipping.address.state)

# # plot distr hhsize for top 25 states
# data |>
# 	filter(shipping.address.state %in% top_25_states) |>
# 	ggplot(aes(y = fct_rev(fct_infreq(shipping.address.state)), x = q.amazon.use.hh.size.num)) +
# 	geom_violin() +
# 	labs(title = "Average Housing Size by State", y = "State", x = "Mean Housing Size")

# diff attempt at plotting per-state distr of hhsize
state_order <- data %>%
  group_by(shipping.address.state) %>%
  summarize(prop_size_1 = mean(q.amazon.use.hh.size.num == 1)) %>% 
  arrange(desc(prop_size_1)) %>%  # Use desc(prop_size_2) if you want top-to-bottom
  pull(shipping.address.state)

data |> mutate(shipping.address.state = factor(shipping.address.state, levels = state_order)) |> 
  ggplot(aes(
		x = shipping.address.state,
		fill = q.amazon.use.hh.size.num,
		group = as.factor(q.amazon.use.hh.size.num)
)) +
	geom_bar(position = position_fill(reverse = TRUE)) +
	scale_fill_gradient(low="#ccebc5", high="#08589e") +
	coord_flip() +
	labs(y = "Proportion", x = "Shipping Address", fill = "HH Size")
```

Interested in seeing if the state/location would show any noticeable patterns in the housing size, our visual showed that the state doesn't seem to make a difference on the average housing size. This suggests that more populated states might not be very important to consider in our final model. Most states tend to average around 2 to 3 for their household size. Hawaii surprisingly shows the highest average while Puerto Rico shows the smallest, yet they are both very small territories.  

### Product Categories

```{r}
# plot correlated cat frequencies against hhsize

ggplot(
  data |> filter(freq_Toys_and_Hobbies > 0), 
  aes(x = q.amazon.use.hh.size.num, y = freq_Toys_and_Hobbies)) +
	geom_jitter(alpha = 0.5)

ggplot(data |> filter(freq_Baby_Product > 0), 
aes(x = q.amazon.use.hh.size.num, y = freq_Baby_Product)) +
	geom_jitter(alpha = 0.5)
```

Certain households would be more likely to purchase products from specific product categories like households with children would be like to purchase products related to toys. Looking at the top 10 purchased product categories, household sizes closer to 3 purchase from these categories, but we noticed that the pet food category showed a household size closer to 2. This makes us believe that there could be certain categories that may be able to predict smaller household sizes better than others. 

### Gender

```{r}
ggplot(data, aes(x = q.demos.gender, y = q.amazon.use.hh.size.num)) +
  geom_violin() +
  labs(title = "Housing Size vs. Gender ",
       x = "Gender",
       y = "Housing Size") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The demographic-related variables we've explored have shown to be useful predictors so we wanted to see if gender would show any patterns as well. However, the violin plots for "Female" and "Male" show their widest density at exactly the same points: at 2 and 3. The most common household sizes are identical for the two largest gender groups and the violin plots fail to show that one gender group's distribution is systematically larger or smaller than another's. Including gender in our model would likely add very little explanatory power. 


### Order Date

```{r}
date_data <- data %>%
  mutate(
    Order.Date = as.Date(Order.Date),         
    Year = format(Order.Date, "%Y"),
    Month = format(Order.Date, "%m")
  )

date_data %>%
  group_by(Year, Month) %>%
  summarise(mean_housing = mean(q.amazon.use.hh.size.num, na.rm = TRUE)) %>%
  ggplot(aes(x = Month, y = mean_housing, color = Year, group = Year)) +
  geom_line(size = 1) +
  geom_point() +
  labs(
    title = "Monthly Trends in Housing Size by Year",
    x = "Month",
    y = "Average Housing Size"
  ) +
  theme_minimal()
```

After analyzing the average housing size by month and year, the order date variable does not appear to make a significant difference in the average household size. The average housing size across all years remains extremely stable throughout the months and there are no major seasonal spikes or drops where the average housing size changes significantly.


# Model Evaluation and Tuning

```{r}
# defining the task
housing_tsk <- as_task_regr(data, target = "q.amazon.use.hh.size.num")
```

```{r, cache=TRUE}
## 70/30 SPLIT

# set seed for reproducibility
set.seed(101) 

# randomly assign 70% of observations to train and 30% to test
split <- partition(housing_tsk, ratio = 0.70)

# define learner
lrn_lm <- as_learner(
  imputation_pipeline %>>%
    po("encode", method = "one-hot") %>>% ## one-hot encode our categorical variables
    lrn("regr.lm") ## pass cleaned dataset into a linear regression model
)

# train linear regression model on training data only
lrn_lm$train(task = housing_tsk, row_ids = split$train)
lrn_lm$model |> tidy()

# predict test data only
preds <- lrn_lm$predict(task = ames_tsk_small, row_ids = ames_split$test)

# performance on test data
preds$score(measures = msr("regr.mse"))
```


```{r, cache=TRUE}
## K FOLD CROSS VALIDATION

# resampling strategy
cv_5fold <- rsmp("cv", folds = 5) 

# defining the learner
lrn_lm <- as_learner(
  imputation_pipeline %>>%
    po("encode", method = "one-hot") %>>% ## one-hot encode our categorical variables
    lrn("regr.lm") ## pass cleaned dataset into a linear regression model
)

# set seed for reproducibility
set.seed(101) 
rr1 <- resample(task = housing_tsk, learner = lrn_lm, resampling = cv_5fold)
rr1$aggregate(measures = msrs(c("time_train", "time_predict", "time_both", "regr.mse")))
```


# Maureen Random Forest Model ----------------------------------------------------

## Hyperparameter optimization: Random Forest model
```{r}
source("random_forest_model.R")
at$tuning_result
```

Thus, the best hyperparameter for our random forest model is:

- num.trees: 618

- mtry: 4

- min.node.size: 11

- sample.fraction: 0.8449092

## Final model

```{r}
source("rf_final_model.R")
# Load final model and predictions saved earlier
final_model <- readRDS("rf_model_final.rds")
preds_final <- readRDS("rf_predictions_final.rds")

# Define measures
measures <- list(
  msr("regr.rmse"),
  msr("regr.mae"),
  msr("regr.mse")
)

# Print performance
scores <- preds_final$score(measures)
scores

```

## Model interpretation: how your model can explain individual predictions it makes

```{r}
load("processed_data_train.RData")
final_model <- readRDS("rf_model_final.rds")

set.seed(101)
housing_tsk <- as_task_regr(train_data, target = "q.amazon.use.hh.size.num")
split     <- partition(housing_tsk, ratio = 0.8)
train_idx <- split$train
test_idx  <- split$test

X_test <- housing_tsk$data(
  rows = test_idx,
  cols = housing_tsk$feature_names
)
y_test <- housing_tsk$data(
  rows = test_idx,
  cols = housing_tsk$target_names
)[[1]]

pred_fun <- function(model, newdata) {
  model$predict_newdata(newdata)$response
}

predictor <- Predictor$new(
  model = final_model,
  data  = X_test,
  y     = y_test,
  predict.function = pred_fun
)

# pick some interesting test points (rows within X_test)
example_ids <- c(1, 20)

# see their predictions for context
final_model$predict_newdata(X_test[example_ids, ])

# LocalModel = LIME-style local surrogate
set.seed(101)
loc_1 <- LocalModel$new(predictor, x.interest = X_test[example_ids[1], ], k = 5)
loc_2 <- LocalModel$new(predictor, x.interest = X_test[example_ids[2], ], k = 5)

# plots: contribution of top features for each case
plot(loc_1)
plot(loc_2)

```

## Show predictions

```{r}
# Load test data 
load("processed_data_test.RData")

# Get predictions on new data
preds_test <- final_model$predict_newdata(newdata = test_data)

# Turn predictions into a data.table 
preds_dt <- as.data.table(preds_test)

submission <- data.frame(
  Survey.ResponseID        = test_data$survey.response.id,
  Q.amazon.use.hh.size.num = preds_dt$response
)
class(submission)

write.csv(submission, "hh_size_predictions_final.csv", row.names = FALSE)
```

## Variable importance

```{r}
importance <- final_model$importance()
class(importance)   # should be "numeric"
str(importance)

vip_df <- data.frame(
  variable   = names(importance),
  importance = as.numeric(importance)
)

# sort by importance, descending
vip_df <- vip_df[order(-vip_df$importance), ]
# drop survey.response.id
vip_df <- vip_df[vip_df$variable != "survey.response.id", ]

library(ggplot2)

ggplot(vip_df[1:15, ], aes(x = reorder(variable, importance), y = importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(
    title = "Top 15 Most Important Variables (Random Forest)",
    x = "Predictor",
    y = "Impurity-based Importance"
  ) +
  theme_minimal()



```

# End of Maureen Random Forest Model ----------------------------------------------------